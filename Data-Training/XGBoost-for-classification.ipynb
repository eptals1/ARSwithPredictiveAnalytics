{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EXTRACT TEXT FROM FOLDERS OF RESUME AND JOB REQUIREMENTS THEN TRANSPORT IT TO CSV FOR DATA TRAINING PROCESS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import textract\n",
    "import PyPDF2\n",
    "import docx\n",
    "import pandas as pd\n",
    "import pytesseract\n",
    "from PIL import Image\n",
    "\n",
    "# Set Tesseract OCR Path (Windows Only)\n",
    "# Update this if Tesseract is installed in a different location\n",
    "pytesseract.pytesseract.tesseract_cmd = r\"C:\\Program Files\\Tesseract-OCR\\tesseract.exe\"\n",
    "\n",
    "def extract_text_from_docx(file_path):\n",
    "    \"\"\"Extract text from a DOCX file.\"\"\"\n",
    "    try:\n",
    "        doc = docx.Document(file_path)\n",
    "        return \"\\n\".join([para.text for para in doc.paragraphs])\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting text from {file_path} using python-docx: {e}\")\n",
    "        return None\n",
    "\n",
    "def extract_text_from_pdf(file_path):\n",
    "    \"\"\"Extract text from a PDF file using PyPDF2.\"\"\"\n",
    "    try:\n",
    "        with open(file_path, \"rb\") as file:\n",
    "            reader = PyPDF2.PdfReader(file)\n",
    "            text = \"\\n\".join([page.extract_text() for page in reader.pages if page.extract_text()])\n",
    "            return text if text else None\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting text from {file_path} using PyPDF2: {e}\")\n",
    "        return None\n",
    "\n",
    "def extract_text_with_textract(file_path):\n",
    "    \"\"\"Fallback text extraction using Textract for unsupported formats.\"\"\"\n",
    "    try:\n",
    "        return textract.process(file_path).decode(\"utf-8\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting text from {file_path} using textract: {e}\")\n",
    "        return None\n",
    "\n",
    "def extract_text_from_image(file_path):\n",
    "    \"\"\"Extract text from an image file using Tesseract OCR.\"\"\"\n",
    "    try:\n",
    "        image = Image.open(file_path)\n",
    "        text = pytesseract.image_to_string(image, config='--psm 6')\n",
    "        return text.strip() if text else None\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting text from {file_path} using OCR: {e}\")\n",
    "        return None\n",
    "\n",
    "def extract_text(file_path):\n",
    "    \"\"\"Detect file type and extract text accordingly.\"\"\"\n",
    "    text = None\n",
    "\n",
    "    if file_path.endswith(\".docx\"):\n",
    "        text = extract_text_from_docx(file_path)\n",
    "    elif file_path.endswith(\".pdf\"):\n",
    "        text = extract_text_from_pdf(file_path)\n",
    "    elif file_path.endswith((\".jpg\", \".jpeg\", \".png\")):\n",
    "        text = extract_text_from_image(file_path)\n",
    "    else:\n",
    "        print(f\"Unsupported file format: {file_path}\")\n",
    "        return None\n",
    "    \n",
    "    # Fallback to Textract if other methods fail\n",
    "    if text is None:\n",
    "        text = extract_text_with_textract(file_path)\n",
    "    \n",
    "    return text\n",
    "\n",
    "def process_files(resume_folder, job_folder, output_csv):\n",
    "    data = []\n",
    "    \n",
    "    resumes = []\n",
    "    for filename in os.listdir(resume_folder):\n",
    "        file_path = os.path.join(resume_folder, filename)\n",
    "        if os.path.isfile(file_path) and file_path.endswith((\".pdf\", \".docx\")):\n",
    "            print(f\"Extracting text from resume: {filename}\")\n",
    "            text = extract_text(file_path)\n",
    "            if text:\n",
    "                resumes.append(preprocess_text(text))\n",
    "    \n",
    "    job_requirements = []\n",
    "    for filename in os.listdir(job_folder):\n",
    "        file_path = os.path.join(job_folder, filename)\n",
    "        if os.path.isfile(file_path) and file_path.endswith((\".pdf\", \".docx\")):\n",
    "            print(f\"Extracting text from job requirement: {filename}\")\n",
    "            text = extract_text(file_path)\n",
    "            if text:\n",
    "                job_requirements.append(preprocess_text(text))\n",
    "    \n",
    "    # Create all possible resume-job requirement pairs and compute Jaccard similarity\n",
    "    for resume_text in resumes:\n",
    "        for job_text in job_requirements:\n",
    "            jaccard_score_value = calculate_jaccard_similarity(resume_text, job_text)\n",
    "            data.append({\n",
    "                \"Resume Text\": resume_text,\n",
    "                \"Job Requirement Text\": job_text,\n",
    "                \"Jaccard Score\": jaccard_score_value\n",
    "            })\n",
    "    \n",
    "    # Save to CSV\n",
    "    df = pd.DataFrame(data)\n",
    "    df.to_csv(output_csv, index=False, encoding=\"utf-8\")\n",
    "    print(f\"Extraction complete. Data saved to {output_csv}\")\n",
    "\n",
    "# Example usage\n",
    "resume_folder = \"resumes\"  # Change this to your resume folder path\n",
    "job_folder = \"job_requirements\"  # Change this to your job requirements folder path\n",
    "output_csv = \"scored_dataset.csv\"\n",
    "process_files(resume_folder, job_folder, output_csv)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PRE-PROCESSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()  # Convert to lowercase\n",
    "    text = re.sub(r\"[^a-zA-Z0-9\\s]\", \"\", text)  # Remove special characters\n",
    "    text = \" \".join([word for word in text.split() if word not in stop_words])  # Remove stop words\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SCORE RESUMES USING JACCARD SIMILARITY BASED ON JOB REQUIREMENTS READY FOR AUTO LABELLING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import jaccard_score\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "def calculate_jaccard_similarity(text1, text2):\n",
    "    vectorizer = CountVectorizer(binary=True)\n",
    "    X = vectorizer.fit_transform([text1, text2]).toarray()\n",
    "    return jaccard_score(X[0], X[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AUTO-LABELLING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "\n",
    "# Load the extracted text dataset\n",
    "df = pd.read_csv(\"scored_dataset.csv\")\n",
    "\n",
    "# If Jaccard Score is <= 0, label as 0 (Not Suitable), else 1 (Suitable)\n",
    "df[\"Label\"] = df[\"Jaccard Score\"].apply(lambda x: 1 if x > 0.5 else 0)\n",
    "\n",
    "# Save the labeled dataset / Labelled for XGBoost Classification\n",
    "df.to_csv(\"labeled_data.csv\", index=False)\n",
    "\n",
    "# Display label distribution\n",
    "print(df[\"Label\"].value_counts())  # Check how many are suitable (1) vs. not suitable (0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Acer\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\xgboost\\core.py:158: UserWarning: [18:50:27] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"n_estimators\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8288639687957094\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.83      0.84      1099\n",
      "           1       0.81      0.83      0.82       952\n",
      "\n",
      "    accuracy                           0.83      2051\n",
      "   macro avg       0.83      0.83      0.83      2051\n",
      "weighted avg       0.83      0.83      0.83      2051\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Load the labeled dataset\n",
    "df = pd.read_csv(\"labeled_data.csv\")\n",
    "\n",
    "# Feature extraction: Use TF-IDF to convert text into numerical format\n",
    "vectorizer = TfidfVectorizer(max_features=5000)  # Limit to 5000 features for efficiency\n",
    "X = vectorizer.fit_transform(df[\"Resume Text\"] + \" \" + df[\"Job Requirement Text\"])  # Combine both texts\n",
    "y = df[\"Label\"]  # Target variable (0 = Not Suitable, 1 = Suitable)\n",
    "\n",
    "# Split data into training and testing sets (80% train, 20% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert to DMatrix for XGBoost\n",
    "dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "dtest = xgb.DMatrix(X_test, label=y_test)\n",
    "\n",
    "# Train XGBoost model\n",
    "params = {\n",
    "    \"objective\": \"binary:logistic\",  # Binary classification\n",
    "    \"eval_metric\": \"logloss\",\n",
    "    \"max_depth\": 6,\n",
    "    \"learning_rate\": 0.1,\n",
    "    \"n_estimators\": 100,\n",
    "}\n",
    "model = xgb.train(params, dtrain, num_boost_round=100)\n",
    "\n",
    "# Predict on test data\n",
    "y_pred = model.predict(dtest)\n",
    "y_pred = [1 if p >= 0.5 else 0 for p in y_pred]\n",
    "\n",
    "# Evaluate model\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Save the trained model\n",
    "model.save_model(\"xgboost_resume_classifier.json\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ðŸ“Œ Steps to Predict New Resumes\n",
    "\n",
    "âœ… 1. Load Your Trained XGBoost Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "# Load the trained model correctly\n",
    "xgb_model = xgb.Booster()\n",
    "xgb_model.load_model(\"xgboost_resume_classifier.json\")  # Load the JSON model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "âœ… 2. Preprocess New Resumes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    text = text.lower()  \n",
    "    text = re.sub(r\"[^a-zA-Z0-9\\s]\", \"\", text)  \n",
    "    text = \" \".join([word for word in text.split() if word not in stop_words])  \n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "âœ… 3. Compute Jaccard Score Against Job Descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import jaccard_score\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "def compute_jaccard(resume_text, job_text):\n",
    "    vectorizer = CountVectorizer(binary=True)\n",
    "    text_data = [resume_text, job_text]\n",
    "    \n",
    "    # Convert text to binary vectors\n",
    "    binary_vectors = vectorizer.fit_transform(text_data).toarray()\n",
    "    \n",
    "    # Compute Jaccard score\n",
    "    return jaccard_score(binary_vectors[0], binary_vectors[1])\n",
    "\n",
    "# Apply Jaccard similarity for new resumes\n",
    "new_resumes[\"Jaccard Score\"] = new_resumes.apply(\n",
    "    lambda row: compute_jaccard(row[\"Resume Text\"], row[\"Job Requirement Text\"]), axis=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "âœ… 4. Predict Suitability (0 = Not Suitable, 1 = Suitable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the feature used in training (Jaccard Score)\n",
    "X_new = new_resumes[[\"Jaccard Score\"]]\n",
    "\n",
    "# Predict using the trained model\n",
    "new_resumes[\"Prediction\"] = xgb_model.predict(X_new)\n",
    "\n",
    "# Map Predictions\n",
    "new_resumes[\"Prediction Label\"] = new_resumes[\"Prediction\"].map({0: \"Not Suitable\", 1: \"Suitable\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "âœ… 5. Save the Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_resumes.to_csv(\"predicted_resumes.csv\", index=False)\n",
    "print(\"Predictions saved to predicted_resumes.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To implement job recommendations for rejected resumes, you can follow these steps:\n",
    "\n",
    "Identify Rejected Resumes: Filter resumes where the model predicts them as \"Not Suitable\" (label = 0).\n",
    "Compare with Other Job Descriptions: Compute similarity scores (e.g., Jaccard, cosine similarity) between rejected resumes and other job descriptions.\n",
    "Rank Suitable Jobs: Sort job descriptions based on similarity scores and suggest the top matches.\n",
    "Save Recommendations: Store job suggestions in a CSV or database for further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Load datasets\n",
    "resumes_df = pd.read_csv(\"resumes.csv\")  # Your dataset containing resumes and suitability labels\n",
    "jobs_df = pd.read_csv(\"job_descriptions.csv\")  # Your dataset containing job descriptions\n",
    "\n",
    "# Filter out rejected resumes\n",
    "rejected_resumes = resumes_df[resumes_df[\"Label\"] == 0]\n",
    "\n",
    "# TF-IDF Vectorization\n",
    "vectorizer = TfidfVectorizer(stop_words='english')\n",
    "resume_vectors = vectorizer.fit_transform(rejected_resumes[\"Resume Text\"])\n",
    "job_vectors = vectorizer.transform(jobs_df[\"Job Requirement Text\"])\n",
    "\n",
    "# Compute Cosine Similarity\n",
    "similarity_matrix = cosine_similarity(resume_vectors, job_vectors)\n",
    "\n",
    "# Generate job recommendations\n",
    "job_suggestions = []\n",
    "for i, resume in rejected_resumes.iterrows():\n",
    "    top_indices = similarity_matrix[i].argsort()[::-1][:3]  # Get top 3 job matches\n",
    "    suggested_jobs = jobs_df.iloc[top_indices][\"Job Requirement Text\"].tolist()\n",
    "    \n",
    "    job_suggestions.append({\n",
    "        \"Resume\": resume[\"Resume Text\"],\n",
    "        \"Suggested Jobs\": suggested_jobs\n",
    "    })\n",
    "\n",
    "# Save recommendations\n",
    "recommendations_df = pd.DataFrame(job_suggestions)\n",
    "recommendations_df.to_csv(\"job_recommendations.csv\", index=False)\n",
    "\n",
    "print(\"Job recommendations saved to job_recommendations.csv\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
